{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import einops\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as ticker\n",
    "import unicodedata\n",
    "import tensorflow as tf \n",
    "import tensorflow.compat.v1 as tf_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D:\\Datasets\\eng_-french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for sentences_eng in dataset['English words/sentences'][:100000]:\n",
    "    input_text.append(sentences_eng)\n",
    "\n",
    "for sentences_fre in dataset['French words/sentences'][:100000]:\n",
    "    sentences_fre = sentences_fre\n",
    "    target_text.append(sentences_fre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = np.array(input_text)\n",
    "french = np.array(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_text)\n",
    "batch_size = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_text),)) < 0.8\n",
    "\n",
    "# train_raw = (tf.data.Dataset.from_tensor_slices((english[is_train],french[is_train])).shuffle(buffer_size).batch(batch_size))\n",
    "train_raw = (tf.data.Dataset.from_tensor_slices((english[is_train],french[is_train])).shuffle(buffer_size).batch(batch_size))\n",
    "\n",
    "# val_raw = (tf.data.Dataset.from_tensor_slices((english[~is_train],french[~is_train])).shuffle(buffer_size).batch(batch_size))\n",
    "val_raw = (tf.data.Dataset.from_tensor_slices((english[~is_train],french[~is_train])).shuffle(buffer_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the text is not done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(txt):\n",
    "    text = tf.strings.lower(txt)\n",
    "    text = tf.strings.regex_replace(text,'[^ a-z.?!,¿]','')\n",
    "    # add spaces around punctuations\n",
    "    text = tf.strings.regex_replace(text,'[.?!,¿]', r'\\0')\n",
    "    # Strip white space\n",
    "    text = tf.strings.strip(text)\n",
    "    \n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text vectorisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocublary_size = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=vocublary_size, ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "a = context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=vocublary_size, ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'je', 'de', 'pas', '?', 'ne', 'que']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "\n",
    "# first 10 words from vocublary:\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    # context = tf.convert_to_tensor(context_text_processor(context))\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   12    5 2531    1    3    0    0    0]\n",
      "\n",
      "[   2 2189  298 1839    7    0    0    0    0    0]\n",
      "[2189  298 1839    7    3    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "    print(ex_context_tok[0,:10].numpy())\n",
    "    print()\n",
    "    print(ex_tar_in[0, :10].numpy()) \n",
    "    print(ex_tar_out[0, :10].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing the encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.units = units\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units, mask_zero = True)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.GRU(units,\n",
    "                                # Return the sequence and state\n",
    "                                return_sequences=True,\n",
    "                                recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "\n",
    "def call(self, x):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x,'batch s')\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "\n",
    "    x = self.rnn(x)\n",
    "    shape_checker(x, 'batch s units')\n",
    "    return x\n",
    "\n",
    "def convert_input(self, texts):\n",
    "    texts = tf.convert_to_tensor(texts)\n",
    "    if len(texts.shape) == 0:\n",
    "        texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "    context = text_preprocessor(texts).to_tensor()\n",
    "    context = self(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 9)\n",
      "Context tokens, shape (batch, s): (64, 9)\n",
      "Encoder output, shape (batch, s, units): (64, 9)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(context_text_processor,UNITS)\n",
    "ex_cont = encoder(ex_context_tok)\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding the Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "\n",
    "    def call(self, x, context):\n",
    "        # shape_checker = ShapeChecker()\n",
    "        # shape_checker(x, 'batch t units')\n",
    "        # shape_checker(context, 'batch s units')\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query = x,\n",
    "            value = context,\n",
    "            return_attention_scores = True\n",
    "        )\n",
    "\n",
    "        # shape_checker(x, 'batch t units')\n",
    "        # shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        shape_checker(attn_scores, 'batch t s')\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'key' (type EinsumDense).\n\ncannot compute Einsum as input #1(zero-based) was expected to be a int64 tensor but is a float tensor [Op:Einsum]\n\nCall arguments received by layer 'key' (type EinsumDense):\n  • inputs=tf.Tensor(shape=(64, 9), dtype=int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m embd \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(target_text_processor\u001b[39m.\u001b[39mvocabulary_size(), output_dim \u001b[39m=\u001b[39m UNITS, mask_zero \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m ex_tar_embed \u001b[39m=\u001b[39m embd(ex_tar_in)\n\u001b[1;32m----> 6\u001b[0m result \u001b[39m=\u001b[39m attention_layer(ex_tar_embed, ex_context)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mContext sequence, shape (batch, s, units): \u001b[39m\u001b[39m{\u001b[39;00mex_context\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget sequence, shape (batch, t, units): \u001b[39m\u001b[39m{\u001b[39;00mex_tar_embed\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m, in \u001b[0;36mCrossAttention.call\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, context):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# shape_checker = ShapeChecker()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# shape_checker(x, 'batch t units')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# shape_checker(context, 'batch s units')\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     attn_output, attn_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(\n\u001b[0;32m     15\u001b[0m         query \u001b[39m=\u001b[39;49m x,\n\u001b[0;32m     16\u001b[0m         value \u001b[39m=\u001b[39;49m context,\n\u001b[0;32m     17\u001b[0m         return_attention_scores \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     20\u001b[0m     \u001b[39m# shape_checker(x, 'batch t units')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[39m# shape_checker(attn_scores, 'batch heads t s')\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     \u001b[39m# Cache the attention scores for plotting later.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     attn_scores \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(attn_scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'key' (type EinsumDense).\n\ncannot compute Einsum as input #1(zero-based) was expected to be a int64 tensor but is a float tensor [Op:Einsum]\n\nCall arguments received by layer 'key' (type EinsumDense):\n  • inputs=tf.Tensor(shape=(64, 9), dtype=int64)"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "embd = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim = UNITS, mask_zero = True)\n",
    "\n",
    "ex_tar_embed = embd(ex_tar_in)\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
